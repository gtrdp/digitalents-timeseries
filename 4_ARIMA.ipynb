{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "Simple minimal working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ARIMA example\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # contrived dataset\n",
    "# data = [x + random() for x in range(1, 100)]\n",
    "\n",
    "data = pd.read_csv('ads.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "# currency = pd.read_csv('currency.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# make prediction\n",
    "yhat = model_fit.predict(1, len(data) + 1, typ='levels')\n",
    "\n",
    "error = data.Ads - yhat\n",
    "print(error)\n",
    "\n",
    "# print(yhat)\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.plot(data, label='Actual')\n",
    "# plt.plot(yhat, label='Predicted')\n",
    "# plt.title('ARIMA Prediction')\n",
    "# plt.grid(True)\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n",
    "\n",
    "Before we start modeling, we should mention such an important property of time series: [**stationarity**](https://en.wikipedia.org/wiki/Stationary_process).\n",
    "\n",
    "If a process is stationary, that means it does not change its statistical properties over time, namely its mean and variance. (The constancy of variance is called [homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity))The covariance function does not depend on time; it should only depend on the distance between observations. You can see this visually on the images in the post by [Sean Abu](http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n",
    "\n",
    "- The red graph below is not stationary because the mean increases over time.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n",
    "\n",
    "- We were unlucky with the variance and see the varying spread of values over time\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n",
    "\n",
    "- Finally, the covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice that the spread becomes closer as time increases. Hence, the covariance is not constant with time in the right chart.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n",
    "\n",
    "So why is stationarity so important? Because it is easy to make predictions on a stationary series since we can assume that the future statistical properties will not be different from those currently observed. Most of the time-series models, in one way or the other, try to predict those properties (mean or variance, for example). Furture predictions would be wrong if the original series were not stationary. Unfortunately, most of the time series that we see outside of textbooks are non-stationary, but we can (and should) change this.\n",
    "\n",
    "So, in order to combat non-stationarity, we have to know our enemy, so to speak. Let's see how we can detect it. We will look at white noise and random walks to learn how to get from one to another for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # more plots\n",
    "\n",
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import statsmodels.formula.api as smf            # statistics and econometrics\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product                    # some useful functions\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "ads = pd.read_csv('ads.csv', index_col=['Time'], parse_dates=['Time'])\n",
    "currency = pd.read_csv('currency.csv', index_col=['Time'], parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
    "    \"\"\"\n",
    "        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n",
    "        \n",
    "        y - timeseries\n",
    "        lags - how many lags to include in ACF, PACF calculation\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsplot(ads.Ads, lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the initial series are stationary; the Dickey-Fuller test rejected the null hypothesis that a unit root is present. Actually, we can see this on the plot itself -- we do not have a visible trend, so the mean is constant and the variance is pretty much stable. The only thing left is seasonality, which we have to deal with prior to modeling. To do so, let's take the \"seasonal difference\", which means a simple subtraction of the series from itself with a lag that equals the seasonal period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_diff = ads.Ads - ads.Ads.shift(24)\n",
    "tsplot(ads_diff[24:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now much better with the visible seasonality gone. However, the autocorrelation function still has too many significant lags. To remove them, we'll take first differences, subtracting the series from itself with lag 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ads_diff = ads_diff - ads_diff.shift(1)\n",
    "tsplot(ads_diff[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Our series now looks like something undescribable, oscillating around zero. The Dickey-Fuller test indicates that it is stationary, and the number of significant peaks in ACF has dropped. We can finally start modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA-family Crash-Course\n",
    "\n",
    "We will explain this model by building up letter by letter. $SARIMA(p, d, q)(P, D, Q, s)$, Seasonal Autoregression Moving Average model:\n",
    "\n",
    "- $AR(p)$ - autoregression model i.e. regression of the time series onto itself. The basic assumption is that the current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as $p$. To determine the initial $p$, you need to look at the PACF plot and find the biggest significant lag after which **most** other lags become insignificant.\n",
    "- $MA(q)$ - moving average model. Without going into too much detail, this models the error of the time series, again with the assumption that the current error depends on the previous with some lag, which is referred to as $q$. The initial value can be found on the ACF plot with the same logic as before. \n",
    "\n",
    "Let's combine our first 4 letters:\n",
    "\n",
    "$AR(p) + MA(q) = ARMA(p, q)$\n",
    "\n",
    "What we have here is the Autoregressive–moving-average model! If the series is stationary, it can be approximated with these 4 letters. Let's continue.\n",
    "\n",
    "- $I(d)$ - order of integration. This is simply the number of nonseasonal differences needed to make the series stationary. In our case, it's just 1 because we used first differences. \n",
    "\n",
    "Adding this letter to the four gives us the $ARIMA$ model which can handle non-stationary data with the help of nonseasonal differences. Great, one more letter to go!\n",
    "\n",
    "- $S(s)$ - this is responsible for seasonality and equals the season period length of the series\n",
    "\n",
    "With this, we have three parameters: $(P, D, Q)$\n",
    "\n",
    "- $P$ - order of autoregression for the seasonal component of the model, which can be derived from PACF. But you need to look at the number of significant lags, which are the multiples of the season period length. For example, if the period equals 24 and we see the 24-th and 48-th lags are significant in the PACF, that means the initial $P$ should be 2.\n",
    "\n",
    "- $Q$ - similar logic using the ACF plot instead.\n",
    "\n",
    "- $D$ - order of seasonal integration. This can be equal to 1 or 0, depending on whether seasonal differeces were applied or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to set the initial parameters, let's have a look at the final plot once again and set the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsplot(ads_diff[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p$ is most probably 4 since it is the last significant lag on the PACF, after which, most others are not significant. \n",
    "- $d$ equals 1 because we had first differences\n",
    "- $q$ should be somewhere around 4 as well as seen on the ACF\n",
    "- $P$ might be 2, since 24-th and 48-th lags are somewhat significant on the PACF\n",
    "- $D$ again equals 1 because we performed seasonal differentiation\n",
    "- $Q$ is probably 1. The 24-th lag on ACF is significant while the 48-th is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test various models and see which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting initial values and some bounds for them\n",
    "ps = range(2, 5)\n",
    "d=1 \n",
    "qs = range(2, 5)\n",
    "Ps = range(0, 2)\n",
    "D=1 \n",
    "Qs = range(0, 2)\n",
    "s = 24 # season length is still 24\n",
    "\n",
    "# creating list with all the possible combinations of parameters\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeSARIMA(parameters_list, d, D, s):\n",
    "    \"\"\"\n",
    "        Return dataframe with parameters and corresponding AIC\n",
    "        \n",
    "        parameters_list - list with (p, q, P, Q) tuples\n",
    "        d - integration order in ARIMA model\n",
    "        D - seasonal integration order \n",
    "        s - length of season\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float(\"inf\")\n",
    "\n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        # we need try-except because on some combinations model fails to converge\n",
    "        try:\n",
    "            model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(param[0], d, param[1]), \n",
    "                                            seasonal_order=(param[3], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "        aic = model.aic\n",
    "        # saving best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "\n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    # sorting in ascending order, the lower AIC is - the better\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result_table = optimizeSARIMA(parameters_list, d, D, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the parameters that give the lowest AIC\n",
    "p, q, P, Q = result_table.parameters[0]\n",
    "\n",
    "best_model=sm.tsa.statespace.SARIMAX(ads.Ads, order=(p, d, q), \n",
    "                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the residuals of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsplot(best_model.resid[24+1:], lags=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the residuals are stationary, and there are no apparent autocorrelations. Let's make predictions using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSARIMA(series, model, n_steps):\n",
    "    \"\"\"\n",
    "        Plots model vs predicted values\n",
    "        \n",
    "        series - dataset with timeseries\n",
    "        model - fitted SARIMA model\n",
    "        n_steps - number of steps to predict in the future\n",
    "        \n",
    "    \"\"\"\n",
    "    # adding model values\n",
    "    data = series.copy()\n",
    "    data.columns = ['actual']\n",
    "    data['arima_model'] = model.fittedvalues\n",
    "    # making a shift on s+d steps, because these values were unobserved by the model\n",
    "    # due to the differentiating\n",
    "    data['arima_model'][:s+d] = np.NaN\n",
    "    \n",
    "    # forecasting on n_steps forward \n",
    "    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n",
    "    forecast = data.arima_model.append(forecast)\n",
    "    # calculate error, again having shifted on s+d steps from the beginning\n",
    "#     error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "#     plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    plt.plot(forecast, color='r', label=\"model\")\n",
    "    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n",
    "    plt.plot(data.actual, label=\"actual\")\n",
    "    plt.legend()\n",
    "    plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotSARIMA(ads, best_model, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we got very adequate predictions. Our model was wrong by 4.01% on average, which is very, very good. However, the overall costs of preparing data, making the series stationary, and selecting parameters might not be worth this accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
